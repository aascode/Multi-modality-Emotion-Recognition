{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import Permute, multiply, Dot, LSTM, Input, Flatten, Concatenate, Embedding, Convolution1D,Dropout, Conv2D, Conv1D, Bidirectional, MaxPooling1D, Reshape\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "\n",
    "import librosa\n",
    "import argparse\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras.engine import InputSpec\n",
    "from keras.callbacks import EarlyStopping,TensorBoard, ModelCheckpoint\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras_multi_head import MultiHeadAttention\n",
    "\n",
    "from spektral.layers import GraphConv, GraphAttention\n",
    "from spektral.layers.ops import sp_matrix_to_sp_tensor\n",
    "from spektral.utils import normalized_laplacian\n",
    "\n",
    "import pickle as plk\n",
    "from scipy.sparse import csr_matrix\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "keras.backend.tensorflow_backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_facial_tra = plk.load(open('../data_clean/split/data_visual_tra', 'rb'))\n",
    "data_facial_pre_tra = plk.load(open('../data_clean/split/data_visual_pre_tra', 'rb'))\n",
    "\n",
    "data_facial_tes = plk.load(open('../data_clean/split/data_visual_tes', 'rb'))\n",
    "data_facial_pre_tes = plk.load(open('../data_clean/split/data_visual_pre_tes', 'rb'))\n",
    "\n",
    "\n",
    "# data_utt_ori_tra = plk.load(open('../data_clean/split/data_utt_ori_tra', 'rb'))\n",
    "data_utt_EA_tra = plk.load(open('../data_clean/split/data_utt_EA_tra', 'rb'))\n",
    "# data_utt_ori_pre_tra = plk.load(open('../data_clean/split/data_utt_ori_pre_tra', 'rb'))\n",
    "data_utt_EA_pre_tra = plk.load(open('../data_clean/split/data_utt_EA_pre_tra', 'rb'))\n",
    "\n",
    "data_utt_pre_tes = plk.load(open('../data_clean/split/data_utt_pre_tes', 'rb'))\n",
    "data_utt_tes = plk.load(open('../data_clean/split/data_utt_tes', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "data_txt_tra = plk.load(open('../data_clean/split/data_txt_tra', 'rb'))\n",
    "data_txt_pre_tra = plk.load(open('../data_clean/split/data_txt_pre_tra', 'rb'))\n",
    "\n",
    "data_txt_tes = plk.load(open('../data_clean/split/data_txt_tes', 'rb'))\n",
    "data_txt_pre_tes = plk.load(open('../data_clean/split/data_txt_pre_tes', 'rb'))\n",
    "\n",
    "\n",
    "data_gens_tra = plk.load(open('../data_clean/split/data_gens_tra', 'rb'))\n",
    "data_gens_tes = plk.load(open('../data_clean/split/data_gens_tes_', 'rb'))\n",
    "\n",
    "\n",
    "data_emos_tra = plk.load(open('../data_clean/split/data_emos_tra_', 'rb'))\n",
    "\n",
    "data_emos_tes = plk.load(open('../data_clean/split/data_emos_tra_', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_facial_tra = np.reshape(data_facial_tra, (5810, 512,1))\n",
    "data_facial_pre_tra = np.reshape(data_facial_pre_tra, (5810, 512,1))\n",
    "\n",
    "data_facial_tes = np.reshape(data_facial_tes, (1623, 512,1))\n",
    "data_facial_pre_tes = np.reshape(data_facial_pre_tes, (1623, 512,1))\n",
    "\n",
    "\n",
    "data_utt_EA_tra = np.asarray(data_utt_EA_tra)\n",
    "data_utt_EA_pre_tra = np.asarray(data_utt_EA_pre_tra)\n",
    "\n",
    "data_utt_pre_tes = np.asarray(data_utt_pre_tes)\n",
    "data_utt_tes = np.asarray(data_utt_tes)\n",
    "\n",
    "\n",
    "data_txt_tra = np.asarray(data_txt_tra)\n",
    "data_txt_pre_tra = np.asarray(data_txt_pre_tra)\n",
    "\n",
    "data_txt_tes = np.asarray(data_txt_tes)\n",
    "data_txt_pre_tes = np.asarray(data_txt_pre_tes)\n",
    "\n",
    "\n",
    "data_gens_tra = to_categorical(data_gens_tra, 2)\n",
    "data_gens_tes = to_categorical(data_gens_tes, 2)\n",
    "\n",
    "data_emos_tra = to_categorical(data_emos_tra, 6)\n",
    "data_emos_tes = to_categorical(data_emos_tes, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data_facial_tra', data_facial_tra.shape)\n",
    "print('data_facial_pre_tra', data_facial_pre_tra.shape)\n",
    "\n",
    "print('data_facial_tes', data_facial_tes.shape)\n",
    "print('data_facial_pre_tes', data_facial_pre_tes.shape)\n",
    "\n",
    "print('data_utt_EA_tra', np.shape(data_utt_EA_tra))\n",
    "print('data_utt_EA_pre_tra', np.shape(data_utt_EA_pre_tra))\n",
    "\n",
    "print('data_utt_pre_tes', np.shape(data_utt_pre_tes))\n",
    "print('data_utt_tes', np.shape(data_utt_tes))\n",
    "\n",
    "print('data_txt_tra', np.shape(data_txt_tra))\n",
    "print('data_txt_pre_tra', np.shape(data_txt_pre_tra))\n",
    "\n",
    "print('data_txt_tes', np.shape(data_txt_tes))\n",
    "print('data_txt_pre_tes', np.shape(data_txt_pre_tes))\n",
    "\n",
    "print('data_gens_tra', np.shape(data_gens_tra))\n",
    "print('data_gens_tes', np.shape(data_gens_tes))\n",
    "\n",
    "print('data_emos_tra', np.shape(data_emos_tra))\n",
    "print('data_emos_tes', np.shape(data_emos_tes))\n",
    "\n",
    "print('data_emos_next_tra', np.shape(data_emos_next_tra))\n",
    "print('data_emos_next_tes', np.shape(data_emos_next_tes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainable = True\n",
    "\n",
    "facial_model = load_model('./facial/facial_model.h5', \n",
    "                          custom_objects={'MultiHeadAttention':MultiHeadAttention, 'EmoEncDec': EmoEncDec})\n",
    "\n",
    "for layer in facial_model.layers:\n",
    "    layer.name = layer.name + str(\"_facial\")\n",
    "    layer.trainable = Trainabl\n",
    "    \n",
    "audio_model = load_model('./audio/audio_model.h5', \n",
    "                          custom_objects={'MultiHeadAttention':MultiHeadAttention, 'EmoEncDec': EmoEncDec})\n",
    "\n",
    "for layer in audio_model.layers:\n",
    "    layer.name = layer.name + str(\"_audio\")\n",
    "    layer.trainable = Trainable\n",
    "    \n",
    "text_model = load_model('./text/text_model.h5', \n",
    "                          custom_objects={'MultiHeadAttention':MultiHeadAttention})\n",
    "\n",
    "for layer in text_model.layers:\n",
    "    layer.name = layer.name + str(\"_text\")\n",
    "    layer.trainable = Trainable\n",
    "\n",
    "facial_model.summary()\n",
    "audio_model.summary()\n",
    "text_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 6\n",
    "batch = 16\n",
    "epochs = 5000\n",
    "l2_reg = 5e-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 6\n",
    "batch = 16\n",
    "epochs = 5000\n",
    "l2_reg = 5e-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_inter_in = Input(tensor=A_inter, sparse=False)\n",
    "\n",
    "\n",
    "facial_hid = facial_model.layers[-2].output\n",
    "audio_hid = audio_model.layers[-3].output\n",
    "text_hid = text_model.layers[-3].output\n",
    "\n",
    "text_hid = Dense(64)(text_hid)\n",
    "\n",
    "merge = Concatenate(axis=-1)([facial_hid, audio_hid, text_hid])\n",
    "\n",
    "merge_inter= Reshape((3,64))(merge)\n",
    "\n",
    "gat_inter_inter  = GraphAttention(32,\n",
    "                                   attn_heads=8,\n",
    "                                   concat_heads=True,\n",
    "                                   dropout_rate=0.3,\n",
    "                                   activation='elu',\n",
    "                                   kernel_regularizer=l2(l2_reg),\n",
    "                                   attn_kernel_regularizer=l2(l2_reg)\n",
    "                                   )([merge_inter, A_inter_in])\n",
    "R_gra = Flatten()(gat_inter_inter)\n",
    "\n",
    "R = Dense(64)(R_gra)\n",
    "emo = Dense(6, name='emo', activation='softmax')(R)\n",
    "gen = Dense(2, name='gen', activation='softmax')(audio_hid)\n",
    "\n",
    "# R_next = Dense(64)(R_gra)\n",
    "# emo_next = Dense(6, name='emo_next', activation='softmax')(R_next)\n",
    "\n",
    "model = Model(inputs=[facial_model.inputs[0], facial_model.inputs[1],\n",
    "                      audio_model.inputs[0], audio_model.inputs[1],\n",
    "                      text_model.inputs[0], text_model.inputs[1],\n",
    "                      A_inter_in\n",
    "                     ],\n",
    "               outputs=[emo, gen])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_root = './combined/'\n",
    "model_file = file_path_root+'Text_AudioEA_Faceori.h5'\n",
    "callback_list = [\n",
    "                    TensorBoard(log_dir=file_path_root),\n",
    "                    EarlyStopping(\n",
    "                        monitor='val_emo_acc',\n",
    "                        patience=100,\n",
    "                        verbose=1,\n",
    "                        mode='auto'\n",
    "                    ),\n",
    "                    ModelCheckpoint(\n",
    "                        filepath=model_file,\n",
    "                        monitor='val_emo_acc',\n",
    "                        save_best_only='True',\n",
    "                        verbose=1,\n",
    "                        mode='auto',\n",
    "                        period=1\n",
    "                    )\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss={'emo':'categorical_crossentropy',\n",
    "                    'gen':'categorical_crossentropy',\n",
    "                    },\n",
    "              loss_weights={'emo':1.,\n",
    "                            'gen':1.,\n",
    "                            },\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "samples = 1623\n",
    "\n",
    "\n",
    "def batch_generator(X1, X2, X3, X4, X5, X6, Y, batch_size = 3):\n",
    "    indices = np.arange(samples) \n",
    "    batch=[]\n",
    "    while True:\n",
    "            for i in indices:\n",
    "                batch.append(i)\n",
    "                if len(batch)==batch_size:\n",
    "                    yield ([X1[batch], X2[batch], X3[batch], X4[batch], X5[batch], X6[batch], A_inter], Y[batch])\n",
    "                    batch=[]\n",
    "\n",
    "model = load_model(model_file, custom_objects={'MultiHeadAttention':MultiHeadAttention, 'EmoEncDec': EmoEncDec, 'GraphAttention':GraphAttention})\n",
    "predicted_test_labels = model.predict_generator(batch_generator(data_facial_pre_tes, data_facial_tes,\n",
    "                            data_utt_pre_tes, data_utt_tes,\n",
    "                            data_txt_pre_tes, data_txt_tes, data_emos_tes), steps = 541)[0].argmax(axis=1)\n",
    "\n",
    "numeric_test_labels = np.array(data_emos_tes).argmax(axis=1)\n",
    "\n",
    "report_filename = file_path_root+'Results.txt' \n",
    "\n",
    "with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "    print(classification_report(numeric_test_labels, predicted_test_labels_t, target_names = ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']), file=f)\n",
    "print(classification_report(numeric_test_labels, predicted_test_labels_t, target_names = ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']))\n",
    "labels = ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']\n",
    "print('   '+' '.join(labels))\n",
    "cm = confusion_matrix(y_true=numeric_test_labels.tolist(), y_pred=predicted_test_labels_t.tolist())\n",
    "print(cm)\n",
    "\n",
    "\n",
    "nor_cm = []\n",
    "for i in range(6):\n",
    "    row_sum = cm[i].sum()\n",
    "#     print(row_sum)\n",
    "    l_n = []\n",
    "    for j in range(6):\n",
    "#         confusion_mat[i][j] = confusion_mat[i][j]/row_sum\n",
    "        l_n.append(cm[i][j]/row_sum)\n",
    "#         print(confusion_mat[i][j]/row_sum)\n",
    "    nor_cm.append(l_n)\n",
    "    \n",
    "df_cm = pd.DataFrame(nor_cm, index = [i for i in ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']],\n",
    "                  columns = [i for i in ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']])\n",
    "\n",
    "sn.heatmap(df_cm,  annot=True)\n",
    "plt.savefig(file_path_root+'cm.png')\n",
    "\n",
    "cm = np.transpose(cm)\n",
    "\n",
    "nor_cm = []\n",
    "for i in range(6):\n",
    "    row_sum = cm[i].sum()\n",
    "#     print(row_sum)\n",
    "    l_n = []\n",
    "    for j in range(6):\n",
    "#         confusion_mat[i][j] = confusion_mat[i][j]/row_sum\n",
    "        l_n.append(cm[i][j]/row_sum)\n",
    "#         print(confusion_mat[i][j]/row_sum)\n",
    "    nor_cm.append(l_n)\n",
    "    \n",
    "df_cm = pd.DataFrame(nor_cm, index = [i for i in ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']],\n",
    "                  columns = [i for i in ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']])\n",
    "\n",
    "sn.heatmap(df_cm,  annot=True)\n",
    "plt.savefig(file_path_root+'cm_precision.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
