{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import LSTM, Input, Flatten, Concatenate, Embedding, Convolution1D,Dropout, Conv2D, Conv1D, Bidirectional\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras.engine import InputSpec\n",
    "from keras.callbacks import EarlyStopping,TensorBoard, ModelCheckpoint\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras_multi_head import MultiHeadAttention\n",
    "\n",
    "import pickle as plk\n",
    "\n",
    "from utilz import *\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "keras.backend.tensorflow_backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utt_ori_tra = plk.load(open('../data_clean/split/data_utt_ori_tra', 'rb'))\n",
    "data_utt_EA_tra = plk.load(open('../data_clean/split/data_utt_EA_tra', 'rb'))\n",
    "\n",
    "data_utt_ori_pre_tra = plk.load(open('../data_clean/split/data_utt_ori_pre_tra', 'rb'))\n",
    "data_utt_EA_pre_tra = plk.load(open('../data_clean/split/data_utt_EA_pre_tra', 'rb'))\n",
    "\n",
    "data_utt_pre_tes = plk.load(open('../data_clean/split/data_utt_pre_tes', 'rb'))\n",
    "data_utt_tes = plk.load(open('../data_clean/split/data_utt_tes', 'rb'))\n",
    "\n",
    "\n",
    "data_emos_tra = plk.load(open('../data_clean/split/data_emos_tra_', 'rb'))\n",
    "data_gens_tra = plk.load(open('../data_clean/split/data_gens_tra', 'rb'))\n",
    "data_emos_tes = plk.load(open('../data_clean/split/data_emos_tes', 'rb'))\n",
    "data_gens_tes = plk.load(open('../data_clean/split/data_gens_tes_', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "loop_n = {0:3,1:2,2:1,3:2,4:2,5:1}\n",
    "for (utt, utt_pre, emo, gen) in zip(data_utt_ori_tra, data_utt_ori_pre_tra, data_emos_tra, data_gens_tra):\n",
    "    for _ in range(loop_n[emo]):\n",
    "        data.append([utt, utt_pre, emo, gen])\n",
    "for (utt, utt_pre, emo, gen) in zip(data_utt_EA_tra, data_utt_EA_pre_tra, data_emos_tes, data_gens_tes):\n",
    "    for _ in range(loop_n[emo]):\n",
    "        data.append([utt, utt_pre, emo, gen])\n",
    "        \n",
    "np.random.shuffle(data)\n",
    "\n",
    "tra_utt, tra_utt_pre, tra_emos, tra_gens = [], [], [], []\n",
    "for (utt, utt_pre, emo, gen) in data:\n",
    "    tra_utt.append(utt)\n",
    "    tra_utt_pre.append(utt_pre)\n",
    "    tra_emos.append(emo)\n",
    "    tra_gens.append(gen)\n",
    "\n",
    "[tes_utt_pre, tes_utt, tes_emos, tes_gens] = [data_utt_pre_tes, data_utt_tes, \n",
    "                                              data_emos_tes, data_gens_tes]\n",
    "\n",
    "data_type = 'float32'\n",
    "[tra_utt, tra_utt_pre, tra_emos, tra_gens,\n",
    "tes_utt, tes_utt_pre, tes_emos, tes_gens]  = [  np.asarray(tra_utt, data_type),\n",
    "                                                np.asarray(tra_utt_pre, data_type),\n",
    "                                                to_categorical(tra_emos, 6, dtype=data_type),\n",
    "                                                to_categorical(tra_gens, 2, dtype=data_type),\n",
    "                                                                                                             \n",
    "                                                np.asarray(tes_utt, data_type),\n",
    "                                                np.asarray(tes_utt_pre, data_type),\n",
    "                                                to_categorical(tes_emos, 6, dtype=data_type),\n",
    "                                                to_categorical(tes_gens, 2, dtype=data_type) ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256\n",
    "features_number = 384\n",
    "hidden_unit = 512\n",
    "dropout_rate = 0.35\n",
    "lstm_cells = 128\n",
    "classes = 6\n",
    "batch = 32\n",
    "epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_utt = Input((384,256))\n",
    "utt = Input((384,256))\n",
    "\n",
    "Audio_processing = Sequential()\n",
    "Audio_processing.add(Bidirectional(LSTM(lstm_cells, return_sequences=True, recurrent_dropout = 0.2)))\n",
    "Audio_processing.add(MultiHeadAttention(head_num=8))\n",
    "Audio_processing.add(Dropout(dropout_rate))\n",
    "Audio_processing.add(EmoEncDec(lstm_cells,lstm_cells, name='EmoEncDec'))\n",
    "# Audio_processing.add(LSTM(lstm_cells,return_sequences=True, name='EmoEncDec_LSTM'))\n",
    "Audio_processing.add(Dropout(dropout_rate))\n",
    "Audio_processing.add(Flatten())\n",
    "Audio_processing.add(Dense(256))\n",
    "\n",
    "\n",
    "pre_utt_feature = Audio_processing(pre_utt)\n",
    "utt_feature = Audio_processing(utt)\n",
    "\n",
    "merge = Concatenate(axis=-1)([pre_utt_feature, utt_feature])\n",
    "\n",
    "# merge_att = AttentionDecoder(lstm_cells,lstm_cells)(merge)\n",
    "\n",
    "\n",
    "# R = MultiHeadAttention(head_num=8)(merge)\n",
    "# R = Flatten()(merge)\n",
    "R = Dense(64)(merge)\n",
    "emo = Dense(classes, name='emo', activation='softmax')(R)\n",
    "gen = Dense(2, name='gen', activation='softmax')(R)\n",
    "\n",
    "\n",
    "model = Model(inputs=[pre_utt, utt],outputs=[emo, gen])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_root = './audio/'\n",
    "model_file = file_path_root+'audio_model.h5'\n",
    "callback_list = [\n",
    "                    TensorBoard(log_dir=file_path_root),\n",
    "                    EarlyStopping(\n",
    "                        monitor='val_emo_acc',\n",
    "                        patience=100,\n",
    "                        verbose=1,\n",
    "                        mode='auto'\n",
    "                    ),\n",
    "                    ModelCheckpoint(\n",
    "                        filepath=model_file,\n",
    "                        monitor='val_emo_acc',\n",
    "                        save_best_only='True',\n",
    "                        verbose=1,\n",
    "                        mode='auto',\n",
    "                        period=1\n",
    "                    )\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss={'emo':'categorical_crossentropy',\n",
    "                        'gen':'categorical_crossentropy',\n",
    "                    },\n",
    "              loss_weights={'emo':1.,\n",
    "                            'gen':1.,\n",
    "                            },\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = model.fit([tra_utt_pre, tra_utt], \n",
    "          [tra_emos, tra_gens],\n",
    "          batch_size=batch,\n",
    "          epochs=epochs,\n",
    "          callbacks=callback_list,      \n",
    "          validation_data=([tes_utt_pre, tes_utt], \n",
    "                           [tes_emos, tes_gens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "model = load_model(model_file, custom_objects={'MultiHeadAttention':MultiHeadAttention, 'EmoEncDec': EmoEncDec})\n",
    "\n",
    "predicted_test_labels = model.predict([tes_utt_pre, tes_utt])[0].argmax(axis=1)\n",
    "numeric_test_labels = np.array(tes_emos).argmax(axis=1)\n",
    "\n",
    "report_filename = file_path_root+'Results_4digits.txt' \n",
    "\n",
    "with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "    print(classification_report(numeric_test_labels, predicted_test_labels, target_names = ['hap', 'sad', 'neu', 'ang', 'exc', 'fru'], digits=4), file=f)\n",
    "print(classification_report(numeric_test_labels, predicted_test_labels, target_names = ['hap', 'sad', 'neu', 'ang', 'exc', 'fru'], digits=4))\n",
    "labels = ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']\n",
    "print('   '+' '.join(labels))\n",
    "cm = confusion_matrix(y_true=numeric_test_labels.tolist(), y_pred=predicted_test_labels.tolist())\n",
    "print(cm)\n",
    "\n",
    "nor_cm = []\n",
    "for i in range(6):\n",
    "    row_sum = cm[i].sum()\n",
    "#     print(row_sum)\n",
    "    l_n = []\n",
    "    for j in range(6):\n",
    "        l_n.append(cm[i][j]/row_sum)\n",
    "    nor_cm.append(l_n)\n",
    "    \n",
    "df_cm = pd.DataFrame(nor_cm, index = [i for i in ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']],\n",
    "                  columns = [i for i in ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']])\n",
    "\n",
    "sn.heatmap(df_cm,  annot=True)\n",
    "\n",
    "plt.savefig(file_path_root+'cm.jpg')\n",
    "\n",
    "cm = np.transpose(cm)\n",
    "\n",
    "nor_cm = []\n",
    "for i in range(6):\n",
    "    row_sum = cm[i].sum()\n",
    "    l_n = []\n",
    "    for j in range(6):\n",
    "        l_n.append(cm[i][j]/row_sum)\n",
    "    nor_cm.append(l_n)\n",
    "    \n",
    "df_cm = pd.DataFrame(nor_cm, index = [i for i in ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']],\n",
    "                  columns = [i for i in ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']])\n",
    "\n",
    "sn.heatmap(df_cm,  annot=True)\n",
    "plt.savefig(file_path_root+'cm_precision.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
