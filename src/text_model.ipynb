{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import LSTM, Input, Flatten, Concatenate, Embedding, Convolution1D,Dropout, Conv2D, Conv1D, Bidirectional, Permute\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import add, dot, concatenate\n",
    "import argparse\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers, constraints, initializers, activations\n",
    "from keras.layers.recurrent import Recurrent\n",
    "from keras.engine import InputSpec\n",
    "from keras.callbacks import EarlyStopping,TensorBoard, ModelCheckpoint\n",
    "from keras.layers.recurrent import Recurrent\n",
    "\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras_multi_head import MultiHeadAttention\n",
    "\n",
    "import pickle as plk\n",
    "\n",
    "from keras_bert import Tokenizer, extract_embeddings, load_trained_model_from_checkpoint\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "\n",
    "\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "keras.backend.tensorflow_backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout_rate = 0.35\n",
    "lstm_cells = 256\n",
    "classes = 6\n",
    "batch = 32\n",
    "epochs = 500\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "max_sentence_length = 139\n",
    "bert_len = 768\n",
    "len_vocab = 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_txt_tra = plk.load(open('../data_clean/split/data_txt_tra', 'rb'))\n",
    "data_txt_pre_tra = plk.load(open('../data_clean/split/data_txt_pre_tra', 'rb'))\n",
    "\n",
    "data_txt_tes = plk.load(open('../data_clean/split/data_txt_tes', 'rb'))\n",
    "data_txt_pre_tes = plk.load(open('../data_clean/split/data_txt_pre_tes', 'rb'))\n",
    "\n",
    "data_emos_tra = plk.load(open('../data_clean/split/data_emos_tra_', 'rb'))\n",
    "data_emos_tes = plk.load(open('../data_clean/split/data_emos_tes', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "# noise = np.random.normal(0, 0.1, (384, 256))\n",
    "loop_n = {0:3,1:2,2:1,3:2,4:2,5:1}\n",
    "for (utt, utt_pre, emo) in zip(data_txt_tra, data_txt_pre_tra, data_emos_tra):\n",
    "    for _ in range(loop_n[emo]):\n",
    "        data.append([utt, utt_pre, emo])\n",
    "        \n",
    "np.random.shuffle(data)\n",
    "data_txt_tra, data_txt_pre_tra, data_emos_tra = [], [], []\n",
    "for (utt, utt_pre, emo) in data:\n",
    "    data_txt_tra.append(utt)\n",
    "    data_txt_pre_tra.append(utt_pre)\n",
    "    data_emos_tra.append(emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_txt_tra = np.asarray(data_txt_tra)\n",
    "data_txt_pre_tra = np.asarray(data_txt_pre_tra)\n",
    "\n",
    "data_emos_tra = to_categorical(data_emos_tra, 6)\n",
    "data_emos_tes = to_categorical(data_emos_tes, 6)\n",
    "\n",
    "print('data_txt_tra:', np.shape(data_txt_tra))\n",
    "print('data_txt_tes:', np.shape(data_txt_tes))\n",
    "\n",
    "print('data_txt_pre_tra:', np.shape(data_txt_pre_tra))\n",
    "print('data_txt_pre_tes:', np.shape(data_txt_pre_tes))\n",
    "\n",
    "print('data_emos_tra:', np.shape(data_emos_tra))\n",
    "print('data_emos_tes:', np.shape(data_emos_tes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SeMemNN_ser(input_shape, class_count):\n",
    "    decay_steps, warmup_steps = calc_train_steps(\n",
    "        input_shape[0],\n",
    "        batch_size=batch,\n",
    "        epochs=epochs,\n",
    "        warmup_proportion=0.01\n",
    "    )\n",
    "    \n",
    "    pre_txt = Input((max_sentence_length,))\n",
    "    txt = Input((max_sentence_length,))\n",
    "    \n",
    "    input_encoder_m = Sequential()\n",
    "    input_encoder_m.add(Embedding(input_dim=len_vocab,\n",
    "                                  output_dim=256))\n",
    "#     input_encoder_m.add(LSTM(256, return_sequences=True))\n",
    "    input_encoder_m.add(Dropout(Dropout_rate))\n",
    "    # output: (samples, max_len_tra_des_sent, embedding_dim)\n",
    "\n",
    "    input_encoder_c = Sequential()\n",
    "    input_encoder_c.add(Embedding(input_dim=len_vocab,\n",
    "                                  output_dim=max_sentence_length))\n",
    "#     input_encoder_c.add(LSTM(max_sentence_length, return_sequences=True))\n",
    "    input_encoder_c.add(Dropout(Dropout_rate))\n",
    "    # output: (samples, max_len_tra_des_sent, max_len_tra_title_sent)\n",
    "\n",
    "    # embed the title into a sequence of vectors\n",
    "    pre_encoder = Sequential()\n",
    "    pre_encoder.add(Embedding(input_dim=len_vocab,\n",
    "                                   output_dim=256,\n",
    "                                   input_length=max_sentence_length))\n",
    "#     pre_encoder.add(LSTM(256, return_sequences=True))\n",
    "    pre_encoder.add(Dropout(Dropout_rate))\n",
    "    # output: (samples, max_len_tra_title_sent, embedding_dim)\n",
    "\n",
    "\n",
    "    input_encoded_m = input_encoder_m(txt)\n",
    "    input_encoded_c = input_encoder_c(pre_txt)\n",
    "    pre_encoded = pre_encoder(pre_txt)\n",
    "\n",
    "    # shape: (samples, max_len_tra_des_sent, max_len_tra_title_sent)\n",
    "    match = dot([input_encoded_m, pre_encoded], axes=(2, 2))\n",
    "    match = Activation('softmax')(match)\n",
    "\n",
    "    response = add([match, input_encoded_c])  # (samples, max_len_tra_des_sent, max_len_tra_title_sent)\n",
    "    response = Permute((2, 1))(response)  # (samples, max_len_tra_title_sent, max_len_tra_des_sent)\n",
    "\n",
    "    R = concatenate([response, pre_encoded])\n",
    "    # R = Bidirectional(LSTM(256))(R)\n",
    "    R = Bidirectional(LSTM(256,return_sequences=True))(R)\n",
    "    R = MultiHeadAttention(head_num=8)(R)\n",
    "    R = Bidirectional(LSTM(256))(R)\n",
    "\n",
    "    R = Dropout(0.35)(R)\n",
    "    R = Dense(classes)(R)  # (samples, vocab_size)\n",
    "    result = Activation('softmax')(R)\n",
    "\n",
    "\n",
    "    model = Model([pre_txt, txt], result)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['mae', 'mse', 'acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SeMemNN_ser(data_txt_tra.shape, 6)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_root = './text/'\n",
    "model_file = file_path_root+'text_model.h5'\n",
    "callback_list = [\n",
    "                    TensorBoard(log_dir=file_path_root),\n",
    "                    EarlyStopping(\n",
    "                        monitor='val_acc',\n",
    "                        patience=150,\n",
    "                        verbose=1,\n",
    "                        mode='auto'\n",
    "                    ),\n",
    "                    ModelCheckpoint(\n",
    "                        filepath=model_file,\n",
    "                        monitor='val_acc',\n",
    "                        save_best_only='True',\n",
    "                        verbose=1,\n",
    "                        mode='auto',\n",
    "                        period=1\n",
    "                    )\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = model.fit([data_txt_tra, data_txt_pre_tra], \n",
    "          data_emos_tra,\n",
    "          batch_size=batch,\n",
    "          epochs=epochs,\n",
    "          callbacks=callback_list,      \n",
    "          validation_data=([data_txt_tes, data_txt_pre_tes], \n",
    "                                 data_emos_tes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "model = load_model(model_file, custom_objects = {'SeqSelfAttention': SeqSelfAttention, 'MultiHeadAttention':MultiHeadAttention})\n",
    "\n",
    "predicted_test_labels = model.predict([data_txt_tes, data_txt_pre_tes]).argmax(axis=1)\n",
    "numeric_test_labels = np.array(data_emos_tes).argmax(axis=1)\n",
    "\n",
    "report_filename = file_path_root+'result_4digits.txt'\n",
    "\n",
    "with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "    print(classification_report(numeric_test_labels, predicted_test_labels, target_names = ['hap', 'sad', 'neu', 'ang', 'exc', 'fru'], digits=4), file=f)\n",
    "    print(classification_report(numeric_test_labels, predicted_test_labels, target_names = ['hap', 'sad', 'neu', 'ang', 'exc', 'fru'], digits=4))\n",
    "    \n",
    "labels = ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']\n",
    "print('   '+' '.join(labels))\n",
    "cm = confusion_matrix(y_true=numeric_test_labels.tolist(), y_pred=predicted_test_labels.tolist())\n",
    "print(cm)\n",
    "\n",
    "\n",
    "\n",
    "nor_cm = []\n",
    "\n",
    "for i in range(6):\n",
    "    row_sum = cm[i].sum()\n",
    "#     print(row_sum)\n",
    "    l_n = []\n",
    "    for j in range(6):\n",
    "        l_n.append(cm[i][j]/row_sum)\n",
    "    nor_cm.append(l_n)\n",
    "    \n",
    "df_cm = pd.DataFrame(nor_cm, index = [i for i in ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']],\n",
    "                  columns = [i for i in ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']])\n",
    "\n",
    "sn.heatmap(df_cm,  annot=True)\n",
    "plt.savefig(file_path_root+'cm.jpg')\n",
    "\n",
    "cm = np.transpose(cm)\n",
    "\n",
    "nor_cm = []\n",
    "for i in range(6):\n",
    "    row_sum = cm[i].sum()\n",
    "#     print(row_sum)\n",
    "    l_n = []\n",
    "    for j in range(6):\n",
    "        l_n.append(cm[i][j]/row_sum)\n",
    "    nor_cm.append(l_n)\n",
    "    \n",
    "df_cm = pd.DataFrame(nor_cm, index = [i for i in ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']],\n",
    "                  columns = [i for i in ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']])\n",
    "\n",
    "sn.heatmap(df_cm,  annot=True)\n",
    "plt.savefig(file_path_root+'cm_precision.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
